# MCP Semantic Embedding - ì˜ë¯¸ ì„ë² ë”© ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤

## ğŸ“‹ ê°œìš”

**Semantic Embedding MCP**ëŠ” ì†ŒìŠ¤ì½”ë“œë¥¼ ê³ ì°¨ì› ë²¡í„° ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

- **í¬íŠ¸:** 9003 (FastAPI)
- **ëª¨ë¸:** CodeBERT + GraphCodeBERT (ì•™ìƒë¸”)
- **ì¶œë ¥:** 768ì°¨ì› ì„ë² ë”© ë²¡í„° + ìœ ì‚¬ë„ ìŠ¤ì½”ì–´

---

## ğŸ¯ ëª©í‘œ

| í•­ëª© | ì„¤ëª… | í™œìš© |
|------|------|------|
| **ì½”ë“œ ì„ë² ë”©** | ì½”ë“œë¥¼ 768D ë²¡í„°ë¡œ ë³€í™˜ | ìœ ì‚¬ ì½”ë“œ ê²€ìƒ‰, í´ëŸ¬ìŠ¤í„°ë§ |
| **ë²¡í„° ìœ ì‚¬ë„** | ë‘ ì½”ë“œì˜ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚° | ì½”ë“œ ì¬ì‚¬ìš© ì¶”ì²œ, ì¤‘ë³µ íƒì§€ |
| **ë°°ì¹˜ ì„ë² ë”©** | ì—¬ëŸ¬ íŒŒì¼ ë³‘ë ¬ ì„ë² ë”© | ì €ì¥ì†Œ ì „ì²´ ë²¡í„°í™” |

---

## ğŸ“‚ íŒŒì¼ êµ¬ì¡°

```
mcp/semantic_embedding/
â”œâ”€â”€ main.py                 # FastAPI ì„œë²„ ë° ì—”ë“œí¬ì¸íŠ¸ âœ…
â”œâ”€â”€ embedder.py             # CodeEmbedder í´ë˜ìŠ¤ âš ï¸ (40% êµ¬í˜„)
â”œâ”€â”€ models_loader.py        # EmbeddingModelPool âš ï¸
â”œâ”€â”€ requirements.txt        # ì˜ì¡´ì„± âœ…
â”œâ”€â”€ Dockerfile              # ì»¨í…Œì´ë„ˆ ë¹Œë“œ âœ…
â””â”€â”€ README.md               # ì´ ë¬¸ì„œ
```

---

## âš™ï¸ êµ¬í˜„ ìƒíƒœ ë° í•„ìš” ì‘ì—…

### âš ï¸ ê¸´ê¸‰ í•„ìš” ì‘ì—…

#### **Task 1: `embedder.py` - `_generate_embedding()` ë©”ì„œë“œ êµ¬í˜„**

**íŒŒì¼:** `mcp/semantic_embedding/embedder.py`

**í˜„ì¬ ìƒíƒœ:**
```python
def _generate_embedding(self, code: str) -> list[float]:
    # ëœë¤ ë²¡í„° (ë°ëª¨)
    return [random.random() for _ in range(768)]  # âŒ
```

**í•„ìš” ìƒíƒœ:**
```python
def _generate_embedding(self, code: str) -> list[float]:
    """
    ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±

    íŒŒì´í”„ë¼ì¸:
    1. ì½”ë“œ ì „ì²˜ë¦¬ (í† í¬ë‚˜ì´ì§•, ê¸¸ì´ ì œí•œ)
    2. 2ê°œ ëª¨ë¸ ë³‘ë ¬ ì¶”ë¡ 
       - CodeBERT: ê¸°ë³¸ ì˜ë¯¸ ë²¡í„°
       - GraphCodeBERT: ë°ì´í„° íë¦„ ë°˜ì˜ ë²¡í„°
    3. ë²¡í„° ì•™ìƒë¸” (0.5 + 0.5 ê°€ì¤‘ í‰ê· )
    4. L2 ì •ê·œí™” í›„ ë°˜í™˜

    Args:
        code: ì†ŒìŠ¤ ì½”ë“œ ë¬¸ìì—´

    Returns:
        768ì°¨ì› ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„°
    """

    # 1ï¸âƒ£ ì½”ë“œ ì „ì²˜ë¦¬
    tokens = self.tokenizer.encode(
        code,
        max_length=512,
        truncation=True,
        padding=True,
        return_tensors="pt"
    ).to(self.device)

    # 2ï¸âƒ£ 2ê°œ ëª¨ë¸ ë³‘ë ¬ ì¶”ë¡ 
    with torch.no_grad():
        # CodeBERT ì¶”ë¡ 
        codebert_output = self.codebert_model(tokens)
        codebert_embedding = codebert_output.last_hidden_state[:, 0, :]  # [CLS] í† í°

        # GraphCodeBERT ì¶”ë¡ 
        graphcodebert_output = self.graphcodebert_model(tokens)
        graphcodebert_embedding = graphcodebert_output.last_hidden_state[:, 0, :]

    # 3ï¸âƒ£ ì•™ìƒë¸” (ê°€ì¤‘ í‰ê· )
    fused_embedding = 0.5 * codebert_embedding + 0.5 * graphcodebert_embedding

    # 4ï¸âƒ£ L2 ì •ê·œí™”
    normalized_embedding = F.normalize(fused_embedding, p=2, dim=1)

    return normalized_embedding[0].cpu().tolist()
```

#### **Task 2: `models_loader.py` - ëª¨ë¸ í’€ êµ¬í˜„**

**íŒŒì¼:** `mcp/semantic_embedding/models_loader.py`

```python
from transformers import AutoModel, AutoTokenizer
import torch

class EmbeddingModelPool:
    """ì„ë² ë”©ìš© ëª¨ë¸ í’€ ê´€ë¦¬"""

    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.models = {}
        self.tokenizer = None

    def load_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("Loading CodeBERT...")
        self.models["codebert"] = AutoModel.from_pretrained(
            "microsoft/codebert-base"
        ).to(self.device)

        print("Loading GraphCodeBERT...")
        self.models["graphcodebert"] = AutoModel.from_pretrained(
            "microsoft/graphcodebert-base"
        ).to(self.device)

        print("Loading Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")

        # í‰ê°€ ëª¨ë“œ (ë“œë¡­ì•„ì›ƒ ë¹„í™œì„±í™”)
        for model in self.models.values():
            model.eval()

    def get_model(self, model_name: str):
        """íŠ¹ì • ëª¨ë¸ ë°˜í™˜"""
        if model_name not in self.models:
            self.load_models()
        return self.models[model_name]

    def get_tokenizer(self):
        """í† í¬ë‚˜ì´ì € ë°˜í™˜"""
        if self.tokenizer is None:
            self.load_models()
        return self.tokenizer

    def unload_all(self):
        """ëª¨ë“  ëª¨ë¸ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°"""
        self.models.clear()
        self.tokenizer = None
        torch.cuda.empty_cache()
```

---

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### 1. ë‹¨ì¼ ì½”ë“œ ì„ë² ë”©

```bash
POST /embed

{
  "code": "def authenticate_user(username, password): ..."
}

Response:
{
  "code_id": "snippet_001",
  "embedding": [0.123, -0.456, ..., 0.789],  # 768D ë²¡í„°
  "dimension": 768,
  "model": "codebert+graphcodebert"
}
```

### 2. ë°°ì¹˜ ì„ë² ë”©

```bash
POST /batch-embed

{
  "codes": [
    "def func1(): ...",
    "def func2(): ...",
    "class MyClass: ..."
  ]
}

Response:
{
  "embeddings": [
    [0.123, -0.456, ...],
    [0.234, -0.567, ...],
    [0.345, -0.678, ...]
  ],
  "count": 3,
  "execution_time": 2.34
}
```

### 3. ìœ ì‚¬ë„ ê³„ì‚°

```bash
POST /similarity

{
  "code1": "def authenticate(): ...",
  "code2": "def login(): ..."
}

Response:
{
  "similarity_score": 0.8742,  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ [0, 1]
  "code1_id": "auth_fn",
  "code2_id": "login_fn"
}
```

### 4. í—¬ìŠ¤ ì²´í¬

```bash
GET /health

Response:
{
  "status": "healthy",
  "models_loaded": ["codebert", "graphcodebert"],
  "memory_usage_mb": 2048
}
```

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### ë‹¨ë… ì‹¤í–‰

```bash
cd mcp/semantic_embedding
pip install -r requirements.txt
python -m uvicorn main:app --port 9003 --reload
```

### Docker ì‹¤í–‰

```bash
docker build -t fithub-embedding mcp/semantic_embedding/
docker run -p 9003:9003 -e TORCH_HOME=/tmp/torch fithub-embedding
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸

```bash
# í—¬ìŠ¤ ì²´í¬
curl http://localhost:9003/health

# ì„ë² ë”© ìƒì„±
curl -X POST http://localhost:9003/embed \
  -H "Content-Type: application/json" \
  -d '{
    "code": "def hello(): return 42"
  }'

# ìœ ì‚¬ë„ ê³„ì‚°
curl -X POST http://localhost:9003/similarity \
  -H "Content-Type: application/json" \
  -d '{
    "code1": "def add(a, b): return a + b",
    "code2": "def sum(x, y): return x + y"
  }'
```

---

## ğŸ“Š ëª¨ë¸ ì •ë³´

| ëª¨ë¸ | í¬ê¸° | íŠ¹ì§• | ìš©ë„ |
|------|------|------|------|
| **CodeBERT** | 125M | ê¸°ë³¸ ì˜ë¯¸ ë²¡í„° | ì½”ë“œ-ë¬¸ì„œ ì •ë ¬ |
| **GraphCodeBERT** | 125M | ë°ì´í„° íë¦„ ê·¸ë˜í”„ ë°˜ì˜ | ì œì–´ íë¦„ ë¶„ì„ |

**ì•™ìƒë¸” ë°©ì‹:**
```
ìµœì¢… ì„ë² ë”© = 0.5 * CodeBERT + 0.5 * GraphCodeBERT
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ íš¨ìœ¨

```python
# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆê°)
model.eval()

# ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
with torch.no_grad():
    embeddings = model(**inputs)
```

### ë°°ì¹˜ ì²˜ë¦¬

```python
# ì—¬ëŸ¬ ì½”ë“œë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
def batch_embed(self, codes: list) -> list:
    results = []
    for code in codes:
        embedding = self._generate_embedding(code)
        results.append(embedding)
    return results
```

### GPU í™œìš©

```python
# GPU ìë™ ê°ì§€
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
```

---

## ğŸ”— ì˜ì¡´ì„±

```
fastapi==0.104.0
uvicorn==0.24.0
pydantic==2.4.0
torch==2.0.1
transformers==4.34.0
numpy==1.24.0
scipy==1.11.0  # cosine_similarity
```

---

## ğŸ“ ê°œë°œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `_generate_embedding()` ë©”ì„œë“œ êµ¬í˜„
- [ ] CodeBERT ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
- [ ] GraphCodeBERT ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡ 
- [ ] ë²¡í„° ì•™ìƒë¸” (0.5 + 0.5)
- [ ] L2 ì •ê·œí™”
- [ ] models_loader.py ì™„ì„±
- [ ] batch_embed() ìµœì í™”
- [ ] ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
- [ ] ë©”ëª¨ë¦¬ ê´€ë¦¬ (ëª¨ë¸ ì–¸ë¡œë“œ)
- [ ] ë¡œì»¬ í…ŒìŠ¤íŠ¸
- [ ] Docker ë¹Œë“œ

---

**ì°¸ê³ :** ìì„¸í•œ ë‚´ìš©ì€ `IMPLEMENTATION_STATUS.md` â†’ Task 4-1, 4-2ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.