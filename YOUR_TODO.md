# ğŸ¯ ë‹¹ì‹ ì´ í•´ì•¼ í•  ì¼ë“¤ (YOUR_TODO)

í˜„ì¬ **ì‹œìŠ¤í…œ í”„ë ˆì„ì›Œí¬ëŠ” ì™„ì„±**ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” ë‹¹ì‹ ì´ ì§ì ‘ êµ¬í˜„í•´ì•¼ í•˜ëŠ” ì‘ì—…ë“¤ì…ë‹ˆë‹¤.

---

## âœ… ìš°ì„ ìˆœìœ„ë³„ ì‘ì—… ëª©ë¡

### **Phase 1: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •** (í•„ìˆ˜)

#### 1. AI ëª¨ë¸ ì¤€ë¹„
- [ ] **ê° MCPë³„ë¡œ í•„ìš”í•œ ëª¨ë¸ì„ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œ**
  - Summarization: CodeT5+, StarCoder2, CodeLlama, UniXcoder ë“±
  - Structural Analysis: GraphCodeBERT, CodeBERT
  - Semantic Embedding: CodeBERT, CuBERT
  - Repository Analysis: ë¶„ì„ìš© CodeBERT
  - Task Recommender: ì¶”ì²œìš© CodeBERT

**ë°©ë²•:**
```bash
# ìë™ ë‹¤ìš´ë¡œë“œ (ê¶Œì¥)
python setup_models.py --non-interactive

# ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ
python -c "
from transformers import AutoModel, AutoTokenizer
model = AutoModel.from_pretrained('Salesforce/codet5p-base', cache_dir='models/summarization')
tokenizer = AutoTokenizer.from_pretrained('Salesforce/codet5p-base', cache_dir='models/summarization')
"
```

**ê²°ê³¼ í™•ì¸:**
```
models/
â”œâ”€â”€ summarization/
â”‚   â”œâ”€â”€ codet5/
â”‚   â”œâ”€â”€ starcoder2/
â”‚   â”œâ”€â”€ codellama/
â”‚   â””â”€â”€ unixcoder/
â”œâ”€â”€ structural_analysis/
â”‚   â”œâ”€â”€ graphcodebert/
â”‚   â””â”€â”€ codebert/
...
```

---

### **Phase 2: ëª¨ë¸ í†µí•©** (í•µì‹¬ ì‘ì—…)

#### 2. Summarization MCPì— ëª¨ë¸ í†µí•©
**íŒŒì¼:** `mcp/summarization/summarizer.py`

- [ ] `_generate_summary()` ë©”ì„œë“œ êµ¬í˜„
  - í˜„ì¬: íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ìš”ì•½ (ë°ëª¨ìš©)
  - ë³€ê²½: ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„±

**ì˜ˆì‹œ êµ¬í˜„:**
```python
def _generate_summary(self, code: str, model_name: str = "codet5") -> str:
    """ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    if model_name == "codet5":
        model, tokenizer, _ = self.model_pool.get_primary_model()

        # ì½”ë“œ í† í°í™”
        inputs = tokenizer(
            code,
            return_tensors="pt",
            max_length=512,
            truncation=True
        ).to(self.device)

        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            outputs = model(**inputs)

        # ê²°ê³¼ ì²˜ë¦¬ (ë‹¹ì‹ ì˜ ëª¨ë¸ ì¶œë ¥ í˜•ì‹ì— ë§ê²Œ)
        # ì˜ˆ: logits â†’ summary text
        summary = self._decode_output(outputs)

        return summary

    # ë‹¤ë¥¸ ëª¨ë¸ë“¤...
```

#### 3. Structural Analysis MCPì— ëª¨ë¸ í†µí•©
**íŒŒì¼:** `mcp/structural_analysis/analyzer.py`

- [ ] GraphCodeBERTë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•œ ê·¸ë˜í”„ ìƒì„±
  - í˜„ì¬: AST íŒŒì‹±ë§Œ ì‚¬ìš©
  - ë³€ê²½: ëª¨ë¸ì˜ ê·¸ë˜í”„ ì„ë² ë”©ì„ í™œìš©í•œ êµ¬ì¡° ë¶„ì„

**êµ¬í˜„í•  ë©”ì„œë“œ:**
```python
def _enhance_graph_with_embeddings(self, nodes, edges):
    """GraphCodeBERTë¡œ ë…¸ë“œ ì„ë² ë”©ì„ ê°•í™”í•©ë‹ˆë‹¤."""
    # ê° ë…¸ë“œë¥¼ ëª¨ë¸ë¡œ ì„ë² ë”©
    # ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ì—£ì§€ ê°€ì¤‘ì¹˜ ì¡°ì •
    pass
```

#### 4. Semantic Embedding MCPì— ëª¨ë¸ í†µí•©
**íŒŒì¼:** `mcp/semantic_embedding/embedder.py`

- [ ] CodeBERTë¥¼ ì‹¤ì œë¡œ ì‚¬ìš©í•œ ì„ë² ë”© ìƒì„±
  - í˜„ì¬: ì˜ì‚¬ë‚œìˆ˜ ê¸°ë°˜ ì„ë² ë”© (ë°ëª¨ìš©)
  - ë³€ê²½: ì‹¤ì œ ëª¨ë¸ì˜ 768ì°¨ì› ë²¡í„° ìƒì„±

**êµ¬í˜„í•  ë©”ì„œë“œ:**
```python
def _generate_embedding(self, code: str, model_name: str = "codebert") -> List[float]:
    """ì‹¤ì œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤."""

    model, tokenizer, _ = self.model_pool.get_primary_model()

    # í† í°í™”
    inputs = tokenizer(
        code,
        return_tensors="pt",
        max_length=512,
        truncation=True
    ).to(self.device)

    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        outputs = model(**inputs)

    # [CLS] í† í°ì˜ ì„ë² ë”© ì¶”ì¶œ (ë˜ëŠ” í‰ê·  í’€ë§)
    embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()

    return embedding.tolist()
```

#### 5. Repository Analysis MCPì— ëª¨ë¸ í†µí•©
**íŒŒì¼:** `mcp/repository_analysis/analyzer.py`

- [ ] ëª¨ë¸ì„ ì‚¬ìš©í•œ ì €ì¥ì†Œ ìš”ì•½ ê°œì„ 
  - í˜„ì¬: ê¸°ë³¸ í†µê³„ë§Œ ì œê³µ
  - ë³€ê²½: ëª¨ë¸ ê¸°ë°˜ ê±°ì‹œì  ë¶„ì„

#### 6. Task Recommender MCPì— ëª¨ë¸ í†µí•©
**íŒŒì¼:** `mcp/task_recommender/recommender.py`

- [ ] í†µí•© ë¶„ì„ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì¶”ì²œ
  - í˜„ì¬: ê·œì¹™ ê¸°ë°˜ ì¶”ì²œ
  - ë³€ê²½: ëª¨ë¸ ê¸°ë°˜ ë³µì¡ë„ ë¶„ì„ ì¶”ê°€

---

### **Phase 3: í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„** (ì„ íƒì ì´ì§€ë§Œ ê¶Œì¥)

#### 7. Agent Serviceì˜ í‰ê°€ ë©”íŠ¸ë¦­ ê°œì„ 
**íŒŒì¼:** `agent/nodes.py` - `evaluate_node()`

í˜„ì¬ ë©”íŠ¸ë¦­ì€ ë”ë¯¸ ë°ì´í„°ì…ë‹ˆë‹¤. ì‹¤ì œ ë©”íŠ¸ë¦­ êµ¬í˜„:

- [ ] **CodeBLEU** ê³„ì‚°
  ```python
  # CodeBLEU ì ìˆ˜ ê³„ì‚°
  from codebeu import calc_code_bleu
  codebleu = calc_code_bleu(refs=[original], hyps=[summary], lang="python")
  ```

- [ ] **BLEURT** ìŠ¤ì½”ì–´
  ```python
  # BLEURT (í•™ìŠµëœ í‰ê°€ ë©”íŠ¸ë¦­)
  from bleurt import score as bleurt_score
  bleurt = bleurt_score.score(original, summary)
  ```

- [ ] **ROUGE-L** ì ìˆ˜
  ```python
  from rouge_score import rouge_scorer
  rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
  rouge_l = rouge.score(original, summary)
  ```

- [ ] **Graph Edit Distance (GED)**
  ```python
  # ì›ë³¸ ê·¸ë˜í”„ì™€ ìƒì„± ê·¸ë˜í”„ ê°„ í¸ì§‘ ê±°ë¦¬
  import networkx as nx
  ged = nx.graph_edit_distance(original_graph, generated_graph)
  ```

---

### **Phase 4: í…ŒìŠ¤íŠ¸ ë° ê²€ì¦** (ê¶Œì¥)

#### 8. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
**ë””ë ‰í† ë¦¬ ìƒì„±:** `tests/`

- [ ] `tests/test_summarization.py`
  ```python
  def test_summarize_function():
      # í…ŒìŠ¤íŠ¸ ì½”ë“œ ì‘ì„±
      pass
  ```

- [ ] `tests/test_structural_analysis.py`
  - ê·¸ë˜í”„ ìƒì„± ê²€ì¦

- [ ] `tests/test_embeddings.py`
  - ì„ë² ë”© ì°¨ì› ë° ìœ ì‚¬ë„ ê²€ì¦

- [ ] `tests/test_workflow.py`
  - LangGraph ì›Œí¬í”Œë¡œìš° ê²€ì¦

#### 9. í†µí•© í…ŒìŠ¤íŠ¸
- [ ] ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
  ```bash
  python -m pytest tests/ -v
  ```

- [ ] Docker í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸
  ```bash
  docker-compose up -d
  curl http://localhost:8000/health
  ```

---

### **Phase 5: ì„±ëŠ¥ ìµœì í™”** (ì„ íƒì )

#### 10. ëª¨ë¸ ìµœì í™”
- [ ] **ì–‘ìí™” (Quantization)**
  ```python
  # INT8 ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ 30% ê°ì†Œ
  quantized_model = torch.quantization.quantize_dynamic(
      model, {torch.nn.Linear}, dtype=torch.qint8
  )
  ```

- [ ] **ë°°ì¹˜ ì²˜ë¦¬**
  ```python
  # MCP ì—”ë“œí¬ì¸íŠ¸ì— ë°°ì¹˜ ì²˜ë¦¬ ì¶”ê°€
  @app.post("/batch-summarize")
  async def batch_summarize(requests: List[SummarizeRequest]):
      # ì—¬ëŸ¬ ìš”ì²­ì„ í•œë²ˆì— ì²˜ë¦¬
      pass
  ```

- [ ] **ìºì‹±**
  ```python
  # ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ ìºì‹±
  from functools import lru_cache
  @lru_cache(maxsize=1000)
  def _cached_embedding(code_hash):
      pass
  ```

- [ ] **GPU ì§€ì› í™œì„±í™”**
  ```bash
  # docker-compose.ymlì—ì„œ
  environment:
    - DEVICE=cuda
    - CUDA_VISIBLE_DEVICES=0
  ```

#### 11. API ì„±ëŠ¥ ê°œì„ 
- [ ] **ì‘ë‹µ ì••ì¶•**
- [ ] **ìš”ì²­ ê²€ì¦ ê°•í™”**
- [ ] **ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ì¶”ê°€**

---

### **Phase 6: ë°°í¬ ì¤€ë¹„** (ì„ íƒì )

#### 12. í”„ë¡œë•ì…˜ ì„¤ì •
- [ ] **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**
  ```bash
  cp .env.example .env
  # .env íŒŒì¼ ìˆ˜ì •
  ```

- [ ] **ë¡œê¹… ì„¤ì •**
  - êµ¬ì¡°í™”ëœ ë¡œê¹… ì¶”ê°€
  - ë¡œê·¸ ë ˆë²¨ ì¡°ì •

- [ ] **ëª¨ë‹ˆí„°ë§ ì¶”ê°€**
  - Prometheus ë©”íŠ¸ë¦­
  - í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸

#### 13. í´ë¼ìš°ë“œ ë°°í¬
- [ ] **AWS ë°°í¬**
  - ECRì— ì´ë¯¸ì§€ í‘¸ì‹œ
  - ECS/EKSì— ë°°í¬

- [ ] **GCP/Azure ë°°í¬**
  - Cloud Run / Container Instances

---

## ğŸ“‹ êµ¬ì²´ì ì¸ ì‘ì—… íë¦„

### **Step 1: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (1-2ì‹œê°„)**
```bash
# ëª¨ë“  ëª¨ë¸ ìë™ ë‹¤ìš´ë¡œë“œ
python setup_models.py --non-interactive

# ìš©ëŸ‰ í™•ì¸
du -sh models/
```

### **Step 2: Summarization í†µí•© (2-3ì‹œê°„)**
1. `mcp/summarization/summarizer.py` ì—´ê¸°
2. `_generate_summary()` ë©”ì„œë“œ êµ¬í˜„
3. ë¡œì»¬ í…ŒìŠ¤íŠ¸: `python -m pytest tests/test_summarization.py`

### **Step 3: ë‚˜ë¨¸ì§€ MCP í†µí•© (1-2ì‹œê°„ ê°ê°)**
- Structural Analysis
- Semantic Embedding
- Repository Analysis
- Task Recommender

### **Step 4: í‰ê°€ ë©”íŠ¸ë¦­ êµ¬í˜„ (2-3ì‹œê°„)**
- Agentì˜ `evaluate_node()` ë©”ì„œë“œ êµ¬í˜„
- ë©”íŠ¸ë¦­ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜

### **Step 5: í†µí•© í…ŒìŠ¤íŠ¸ (1ì‹œê°„)**
```bash
docker-compose up -d
# API í…ŒìŠ¤íŠ¸
curl -X POST http://localhost:8000/analyze \
  -H "Content-Type: application/json" \
  -d '{"repo": {"source": "local", "uri": "/path/to/test/repo"}}'
```

---

## ğŸ” ëª¨ë¸ í†µí•© ì²´í¬ë¦¬ìŠ¤íŠ¸

ê° MCPì— ëŒ€í•´:

- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- [ ] `models_loader.py`ì—ì„œ ëª¨ë¸ ë¡œë“œ í™•ì¸
- [ ] í•µì‹¬ ë¡œì§ íŒŒì¼ì—ì„œ ëª¨ë¸ ì‚¬ìš© ì½”ë“œ ì‘ì„±
- [ ] ë¡œì»¬ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] Docker í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì‘ë‹µ í˜•ì‹ í™•ì¸

---

## ğŸ“š ì°¸ê³ í•  íŒŒì¼ë“¤

ë‹¹ì‹ ì´ ìˆ˜ì •í•´ì•¼ í•  ì£¼ìš” íŒŒì¼ë“¤:

```
mcp/
â”œâ”€â”€ summarization/
â”‚   â”œâ”€â”€ summarizer.py          â† ìš”ì•½ ë¡œì§ êµ¬í˜„
â”‚   â”œâ”€â”€ models_loader.py        â† ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ëŒ€ë¶€ë¶„ êµ¬í˜„ë¨)
â”‚   â””â”€â”€ main.py                 â† ë³€ê²½ ë¶ˆí•„ìš”
â”‚
â”œâ”€â”€ structural_analysis/
â”‚   â”œâ”€â”€ analyzer.py             â† ê·¸ë˜í”„ ìƒì„± ë¡œì§ êµ¬í˜„
â”‚   â”œâ”€â”€ models_loader.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ semantic_embedding/
â”‚   â”œâ”€â”€ embedder.py             â† ì„ë² ë”© ë¡œì§ êµ¬í˜„
â”‚   â”œâ”€â”€ models_loader.py
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ repository_analysis/
â”‚   â”œâ”€â”€ analyzer.py             â† ì €ì¥ì†Œ ë¶„ì„ ë¡œì§ ê°œì„ 
â”‚   â”œâ”€â”€ models_loader.py
â”‚   â””â”€â”€ main.py
â”‚
â””â”€â”€ task_recommender/
    â”œâ”€â”€ recommender.py          â† ì¶”ì²œ ë¡œì§ ê°œì„ 
    â”œâ”€â”€ models_loader.py
    â””â”€â”€ main.py

agent/
â”œâ”€â”€ nodes.py                    â† evaluate_node() ë©”íŠ¸ë¦­ êµ¬í˜„
â”œâ”€â”€ workflow.py                 â† ë³€ê²½ ë¶ˆí•„ìš”
â”œâ”€â”€ main.py                     â† ë³€ê²½ ë¶ˆí•„ìš”
â””â”€â”€ config.py                   â† í•„ìš”ì‹œ ì„¤ì • ìˆ˜ì •
```

## ğŸ’¡ íŒ

1. **ì‘ì€ ì €ì¥ì†Œë¶€í„° ì‹œì‘**
   - Flask, requests ê°™ì€ ì‘ì€ ì €ì¥ì†Œë¡œ í…ŒìŠ¤íŠ¸

2. **ëª¨ë¸ ì‹¤í–‰ ì†ë„ í™•ì¸**
   - CPUë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸ í›„ GPU í™œì„±í™”

3. **ë””ë²„ê¹…**
   ```bash
   # ê° MCP ë¡œê·¸ í™•ì¸
   docker logs summarization-mcp -f

   # Python ë””ë²„ê±° ì‚¬ìš©
   import pdb; pdb.set_trace()
   ```

4. **ì ì§„ì  êµ¬í˜„**
   - í•œ ë²ˆì— ëª¨ë“  ëª¨ë¸ êµ¬í˜„í•˜ì§€ ë§ê¸°
   - í•˜ë‚˜ì”© ì™„ì„±í•˜ê³  í…ŒìŠ¤íŠ¸

5. **ë¬¸ì„œí™”**
   - ê° ë©”ì„œë“œì— docstring ì¶”ê°€
   - ëª¨ë¸ë³„ íŒŒë¼ë¯¸í„° ë¬¸ì„œí™”

---

## âœ¨ ì™„ë£Œ í›„

ëª¨ë“  ì‘ì—…ì„ ì™„ë£Œí•˜ë©´:

```bash
# ìµœì¢… í…ŒìŠ¤íŠ¸
docker-compose down && docker-compose up -d

# API í…ŒìŠ¤íŠ¸
curl http://localhost:8000/health

# ì²« ë¶„ì„ ì‹¤í–‰
curl -X POST http://localhost:8000/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "repo": {
      "source": "git",
      "uri": "https://github.com/pallets/flask"
    }
  }'

# ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼
pytest tests/ -v --cov
```

---

**ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì´ íŒŒì¼ì˜ í•´ë‹¹ ì„¹ì…˜ì„ ì°¸ê³ í•˜ì„¸ìš”!** ğŸš€
