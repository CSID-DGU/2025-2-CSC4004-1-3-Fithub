"""
agent/config.py
Central configuration with Verified Safe Models for Free Tier.
"""
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    # --- External APIs ---
    HF_API_KEY = os.getenv("HF_API_KEY")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

    # --- Model Provider Settings ---
    # Options: "huggingface", "openai"
    LLM_PROVIDER = os.getenv("LLM_PROVIDER", "huggingface")

    # --- Internal Services ---
    BACKEND_API_URL = os.getenv("BACKEND_API_URL", "http://backend:4000/api")
    GRAPH_MODEL_SERVER_URL = os.getenv("GRAPH_MODEL_SERVER_URL", "http://localhost:9000")

    # --- File System ---
    TEMP_DIR = "./temp_repos"
    LOCAL_MODEL_DIR = "/Users/iyeonglag/PycharmProjects/2025-2-CSC4004-1-3-Fithub/models/RepoGraph"

    # --- Settings ---
    MAX_RETRIES = 2
    TIMEOUT = 60.0
    MAX_ANALYSIS_FILES = 50 # Reduced for testing (was 10000)

    # --- ğŸ¤– Model Configurations (Verified for Free Tier) ---

    # 1. [ìš”ì•½] Qwen/Qwen2.5-Coder-1.5B-Instruct
    # ì´ìœ : ìµœì‹  ì†Œí˜• ì–¸ì–´ ëª¨ë¸ë¡œì„œ HF Inference API (ë¬´ë£Œ) ì§€ì›ì´ ì›í™œí•˜ë©° Chat API í˜¸í™˜ë¨.
    # CodeT5ëŠ” text_generation API í˜¸í™˜ì„± ë¬¸ì œ(StopIteration)ë¡œ êµì²´ë¨.
    MODEL_SUMMARIZER = "Qwen/Qwen2.5-Coder-1.5B-Instruct"

    # 2. [ì„ë² ë”©] microsoft/graphcodebert-base
    # ì´ìœ : ì½”ë“œ ì„ë² ë”©ì˜ í‘œì¤€. Feature Extraction API ì§€ì›ì´ í™•ì‹¤í•¨.
    MODEL_EMBEDDER = "microsoft/graphcodebert-base"

    # 3. [ë¶„ì„/íƒœê¹…]
    MODEL_LLM = "mistralai/Mistral-7B-Instruct-v0.3"
    MODEL_LLM_OPENAI = "gpt-4o" # OpenAI ì‚¬ìš© ì‹œ ê¸°ë³¸ ëª¨ë¸

    # --- ğŸ¤– Ensemble Summarization Models ---
    # Qwenìœ¼ë¡œ í†µì¼ (Role Promptingìœ¼ë¡œ ê´€ì  ë¶„ë¦¬)
    MODEL_SUMMARIZER_LOGIC = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
    MODEL_SUMMARIZER_INTENT = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
    MODEL_SUMMARIZER_STRUCTURE = "Qwen/Qwen2.5-Coder-1.5B-Instruct"
